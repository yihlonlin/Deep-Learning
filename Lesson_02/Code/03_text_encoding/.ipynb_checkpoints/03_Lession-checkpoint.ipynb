{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Hard Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3]\n",
      "(4,)\n",
      "(4, 1)\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "S = np.array(['A', 'B','C','D'])\n",
    "le = LabelEncoder()\n",
    "S = le.fit_transform(S)\n",
    "print(S)\n",
    "ohe = OneHotEncoder(categories='auto')\n",
    "print(S.shape)\n",
    "print(S.reshape(-1,1).shape)\n",
    "one_hot = ohe.fit_transform(S.reshape(-1,1)).toarray()\n",
    "print(one_hot.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brown', 'dog', 'fox', 'jumped', 'lazy', 'over', 'quick', 'the']\n",
      "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
      "(1, 8)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[1 1 1 1 1 1 1 2]]\n",
      "[[0 0 0 0 0 0 4 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "# create the transform\n",
    "vectorizer = CountVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "# summarize\n",
    "print(vectorizer.get_feature_names())\n",
    "print(vectorizer.vocabulary_)\n",
    "# encode document\n",
    "vector = vectorizer.transform(text)\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(type(vector))\n",
    "print(vector.toarray())\n",
    "text1 = [\"quick quick quick quick\"]\n",
    "vector1 = vectorizer.transform(text1)\n",
    "print(vector1.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 8, 'is': 3, 'the': 6, 'first': 2, 'document': 1, 'second': 5, 'and': 0, 'third': 7, 'one': 4}\n",
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 1 0 1 0 2 1 0 1]\n",
      " [1 0 0 0 1 0 1 1 0]\n",
      " [0 1 1 1 0 0 1 0 1]]\n",
      "[[0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.vocabulary_)\n",
    "word = vectorizer.get_feature_names()\n",
    "print(word)\n",
    "print(X.toarray()) #字詞對到一個整數索引\n",
    "X = vectorizer.transform([corpus[0]])\n",
    "print(X.toarray()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this is': 10, 'is the': 2, 'the first': 6, 'first document': 1, 'the second': 7, 'second second': 5, 'second document': 4, 'and the': 0, 'the third': 8, 'third one': 9, 'is this': 3, 'this the': 11}\n",
      "['and the', 'first document', 'is the', 'is this', 'second document', 'second second', 'the first', 'the second', 'the third', 'third one', 'this is', 'this the']\n",
      "[[0 1 1 0 0 0 1 0 0 0 1 0]\n",
      " [0 0 1 0 1 1 0 1 0 0 1 0]\n",
      " [1 0 0 0 0 0 0 0 1 1 0 0]\n",
      " [0 1 0 1 0 0 1 0 0 0 0 1]]\n",
      "[[0 1 1 0 0 0 1 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.vocabulary_)\n",
    "word = vectorizer.get_feature_names()\n",
    "print(word)\n",
    "print(X.toarray()) #字詞對到一個整數索引\n",
    "X = vectorizer.transform([corpus[0]])\n",
    "print(X.toarray()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework\n",
    "# CountVectorizer(ngram_range=(2,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brown fox', 'brown fox jumped', 'brown fox jumped over', 'fox jumped', 'fox jumped over', 'fox jumped over the', 'jumped over', 'jumped over the', 'jumped over the lazy', 'lazy dog', 'over the', 'over the lazy', 'over the lazy dog', 'quick brown', 'quick brown fox', 'quick brown fox jumped', 'the lazy', 'the lazy dog', 'the quick', 'the quick brown', 'the quick brown fox']\n",
      "{'the quick': 18, 'quick brown': 13, 'brown fox': 0, 'fox jumped': 3, 'jumped over': 6, 'over the': 10, 'the lazy': 16, 'lazy dog': 9, 'the quick brown': 19, 'quick brown fox': 14, 'brown fox jumped': 1, 'fox jumped over': 4, 'jumped over the': 7, 'over the lazy': 11, 'the lazy dog': 17, 'the quick brown fox': 20, 'quick brown fox jumped': 15, 'brown fox jumped over': 2, 'fox jumped over the': 5, 'jumped over the lazy': 8, 'over the lazy dog': 12}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "# create the transform\n",
    "vectorizer = CountVectorizer(ngram_range=(2,4))\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "# summarize\n",
    "print(vectorizer.get_feature_names())\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
      "[[1 1 1 1 1 1 1 2]\n",
      " [0 1 0 0 0 0 0 1]\n",
      " [0 0 1 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\", \"The dog.\", \"The fox\"]\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(text)\n",
    "print(vectorizer.vocabulary_)\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
      "['brown', 'dog', 'fox', 'jumped', 'lazy', 'over', 'quick', 'the']\n",
      "[1.69315 1.28768 1.28768 1.69315 1.69315 1.69315 1.69315 1.     ]\n",
      "(3, 8)\n",
      "[[0.36389 0.27675 0.27675 0.36389 0.36389 0.36389 0.36389 0.42983]\n",
      " [0.      0.78981 0.      0.      0.      0.      0.      0.61336]\n",
      " [0.      0.      0.78981 0.      0.      0.      0.      0.61336]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\", \"The dog.\", \"The fox\"]\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "np.set_printoptions(precision=5)\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(vectorizer.idf_) # 整個文件資料\n",
    "vector = vectorizer.transform(text)\n",
    "#summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6931471805599453, 0.28768207245178085, 0.28768207245178085, 0.6931471805599453, 0.6931471805599453, 0.6931471805599453, 0.6931471805599453, 0.0)\n",
      "[1.6931471805599454, 1.2876820724517808, 1.2876820724517808, 1.6931471805599454, 1.6931471805599454, 1.6931471805599454, 1.6931471805599454, 1.0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "#'brown'\n",
    "idf_brown = math.log((1+3)/(1+1))\n",
    "#'dog'\n",
    "idf_dog = math.log((1+3)/(1+2))\n",
    "#'fox'\n",
    "idf_fox = math.log((1+3)/(1+2))\n",
    "#'jumped'\n",
    "idf_jumped = math.log((1+3)/(1+1))\n",
    "#'lazy'\n",
    "idf_lazy = math.log((1+3)/(1+1))\n",
    "#'over':\n",
    "idf_over = math.log((1+3)/(1+1))\n",
    "#'quick':\n",
    "idf_quick = math.log((1+3)/(1+1))\n",
    "#'the'\n",
    "idf_the = math.log((1+3)/(1+3))\n",
    "\n",
    "idf_vec = (idf_brown, idf_dog, idf_fox, idf_jumped, idf_lazy, idf_over, idf_quick, idf_the)\n",
    "print(idf_vec)\n",
    "print([i+1 for i in idf_vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6133555370249717\n",
      "0.7898069290660905\n"
     ]
    }
   ],
   "source": [
    "#computing documentation\n",
    "tf_the_d2=1/2\n",
    "tf_dog_d2=1/2\n",
    "idf_the_v=idf_the+1\n",
    "idf_dog_v=idf_dog+1\n",
    "dis=math.sqrt((tf_the_d2*idf_the_v)*(tf_the_d2*idf_the_v)+(tf_dog_d2*idf_dog_v)*(tf_dog_d2*idf_dog_v))\n",
    "tfidf_the=(tf_the_d2*idf_the_v)/dis\n",
    "tfidf_dog=(tf_dog_d2*idf_dog_v)/dis\n",
    "print(tfidf_the)\n",
    "print(tfidf_dog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework \n",
    "# comput TF-IDF for \"The quick brown fox jumped over the lazy dog.\"\n",
    "ANS: [0.36389 0.27675 0.27675 0.36389 0.36389 0.36389 0.36389 0.42983]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brown : 0.1111111111111111\n",
      "dog : 0.1111111111111111\n",
      "fox : 0.1111111111111111\n",
      "jumped : 0.1111111111111111\n",
      "lazy : 0.1111111111111111\n",
      "over : 0.1111111111111111\n",
      "quick : 0.1111111111111111\n",
      "the : 0.2222222222222222\n"
     ]
    }
   ],
   "source": [
    "import math, re\n",
    "import string\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\", \"The dog.\", \"The fox\"]\n",
    "from nltk.tokenize import word_tokenize\n",
    "# prepare regex for char filtering\n",
    "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "txt =re_punc.sub('', text[0])\n",
    "tokens = word_tokenize(txt)\n",
    "tokens = word_tokenize(txt.lower())\n",
    "words = vectorizer.get_feature_names()\n",
    "for word in words:\n",
    "    if_=tokens.count(word)/len(tokens)\n",
    "    print(word,':',if_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brown : 1.6931471805599454\n",
      "dog : 1.2876820724517808\n",
      "fox : 1.2876820724517808\n",
      "jumped : 1.6931471805599454\n",
      "lazy : 1.6931471805599454\n",
      "over : 1.6931471805599454\n",
      "quick : 1.6931471805599454\n",
      "the : 1.0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "print(\"brown\",':',math.log((3+1)/(1+1))+1)\n",
    "print(\"dog\",':',math.log((3+1)/(2+1))+1)\n",
    "print(\"fox\",':',math.log((3+1)/(2+1))+1)\n",
    "print(\"jumped\",':',math.log((3+1)/(1+1))+1)\n",
    "print(\"lazy\",':',math.log((3+1)/(1+1))+1)\n",
    "print(\"over\",':',math.log((3+1)/(1+1))+1)\n",
    "print(\"quick\",':',math.log((3+1)/(1+1))+1)\n",
    "print(\"the\",':',math.log((3+1)/(3+1))+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20)\n",
      "[[ 0.          0.          0.          0.          0.          0.33333333\n",
      "   0.         -0.33333333  0.33333333  0.          0.          0.33333333\n",
      "   0.          0.          0.         -0.33333333  0.          0.\n",
      "  -0.66666667  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "# create the transform\n",
    "vectorizer = HashingVectorizer(n_features=20)\n",
    "# encode document\n",
    "vector = vectorizer.transform(text)\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
